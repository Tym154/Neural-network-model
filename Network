#include <iostream>
#include <vector>
#include <cmath>
#include <fstream>
#include <sstream>
#include <string>
#include <random>

using namespace std;

int label;

vector<vector<vector<double>>> weights(4); //Storing the weights
vector<vector<double>> nodeVal = {{0},{0},{0},{0}}; //Storning the node values


//Function to read the data line by line from csv file
//and store it into a vector<double> to further process it
vector<double> reading_data(){
    fstream data;
    data.open("data/mnist_train.csv");

    if(!data.is_open()){
        cout << "Error \n";
    }

    string line;
    getline(data, line);

    vector<double> rowData(784, 0);
    string temp = "";
    int index = 0;

    for(int i = 0; i < line.length(); i++){
        if(line[i] == ',' && i == 1){
            label = stod(temp);
            temp = "";
            continue;
        }
        if(line[i] == ','){
            rowData[index++] = stod(temp);
            temp = "";
        }
        else{
            temp += line[i];
        }
    }

    return rowData;
}


//Function to resize the nodeVal so it stores the proper number of node values
void assign(){
    vector<double> layerHid(16, 0), layerOut(10, 0);
    nodeVal[0] = reading_data();
    nodeVal[1] = layerHid;
    nodeVal[2] = layerHid;
    nodeVal[3] = layerOut;
}


//When the layer is hidden i use the Leaky relu function
double leaky_relu(double x){
    return (x > 0) ? x : x * -0.001;
}
//and if the layer is output u use the softmax function (idk if this is right TvT)
vector<double> softmax(vector<double> in){
    double sum = 0.0;

    for(double inputs : in){
        sum += inputs;
    }

    for(int i = 0; i < in.size(); i++){
        in[i] = in[i] / sum;
    }
    return in;
}


//Forward propagation function
void forward_prop(vector<vector<vector<double>>>& weights, vector<vector<double>>& nodeVal){
    for(int i = 0; i < weights.size() - 1; i++){
        vector<double> temp(nodeVal[i+1].size(), 0.0);

        for(int j = 0; j < weights[i].size(); j++){
            for(int k = 0; k < weights[i][j].size(); k++){
                temp[k] += weights[i][j][k] * nodeVal[i][j];
            }
        }

        for(int k = 0; k < temp.size(); k++){
            temp[k] = leaky_relu(temp[k]);
        }

        nodeVal[i+1] = temp;
    }
    nodeVal[nodeVal.size()-1] = softmax(nodeVal[nodeVal.size()-1]);
}


//Assigning random values between -1 and 1 to weights at the begining of the training
void randomWeights(){
    random_device rd; // obtain a random number from hardware
    mt19937 gen(rd()); // seed the generator
    uniform_real_distribution<> distr(-1.0, 1.0);

    for(int i = 0; i < nodeVal.size() - 1; i++){
        weights[i].resize(nodeVal[i].size());

        for(int j = 0; j < nodeVal[i].size(); j++){
            weights[i][j].resize(nodeVal[i+1].size());

            if(i < weights.size() - 1){
                for(int k = 0; k < nodeVal[i+1].size(); k++){
                    weights[i][j][k] = distr(gen);
                }
            }
        }
    }
}



void Display_nodeVals(){
    int count = 0;
    for(int i = 0; i < nodeVal.size(); i++){
        cout << "\n\n" << "Leyer: " << i << "\n";
        for(int j = 0; j < nodeVal[i].size(); j++){
            cout << nodeVal[i][j] << " ";
            count++;
        }
        cout << "\nNumber of nodes in layer: " <<   count <<"\n";
        count = 0;
    }
}



void Display_weights(){
    int count = 0;
    for(int i = 0; i < weights.size(); i++){
        cout << "\n\n" << "Leyer: " << i << "\n";
        for(int j = 0; j < weights[i].size(); j++){
            for(int k = 0; k < weights[i][j].size(); k++){
                cout << weights[i][j][k] << " ";
                count++;
            }
        }
        cout << "\nNumber of weights in layer: " <<count <<"\n\n";
        count = 0;
    }
}

//Just testing the functions inside the main
int main() {
    assign();
    randomWeights();
    forward_prop(weights, nodeVal);

    Display_nodeVals();
    

    cout << "    " << label;

    return 0;
}
